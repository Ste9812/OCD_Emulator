{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fd683b7",
   "metadata": {},
   "source": [
    "# Parameterized Model\n",
    "The following settings prepair the data to create a parameterized model. A parameterization configuration file must be created. Any number of parameters up to 65355-(number of audio chanels) parameters may be defined. The limit is due to the number of channels allowed in a WAV file. \n",
    "\n",
    "The training instructions must then be modified to included the extra inputs to the model as ```-is <parameters + audio channels>``` That is, the number of inputs to the model must be the number of parameters plus the number of audio chanels.\n",
    "\n",
    "As is the default, the training and validation data is split on modulus of 5 which gives a 20 percent split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e3e47f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameterized Data\n",
      "\n",
      "!!WARNING!!\n",
      "nThe test set and validation set are the same!\n",
      "Thus the test results will be significantly biased!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stefano\\Documents\\GitHub\\STMAE_Project\\Training\\prep_wav.py:127: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  in_rate, in_data = wavfile.read(ds[\"TrainingClean\"])\n",
      "c:\\Users\\Stefano\\Documents\\GitHub\\STMAE_Project\\Training\\prep_wav.py:128: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  out_rate, out_data = wavfile.read(ds[\"TrainingTarget\"])\n"
     ]
    }
   ],
   "source": [
    "!python prep_wav.py \"OCD_Parameterized\" -p \"./Configs/Parameterization-Config.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d300edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "existing model file found, loading network\n",
      "cuda device available\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "Epoch:  1\n",
      "current learning rate: 0.005\n",
      "Epoch:  2\n",
      "Val loss: tensor(0.0190)\n",
      "current learning rate: 0.005\n",
      "Epoch:  3\n",
      "current learning rate: 0.005\n",
      "Epoch:  4\n",
      "Val loss: tensor(0.0017)\n",
      "current learning rate: 0.005\n",
      "Epoch:  5\n",
      "current learning rate: 0.005\n",
      "Epoch:  6\n",
      "Val loss: tensor(0.0019)\n",
      "current learning rate: 0.005\n",
      "Epoch:  7\n",
      "current learning rate: 0.005\n",
      "Epoch:  8\n",
      "Val loss: tensor(0.0030)\n",
      "current learning rate: 0.005\n",
      "Epoch:  9\n",
      "current learning rate: 0.005\n",
      "Epoch:  10\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.005\n",
      "Epoch:  11\n",
      "current learning rate: 0.005\n",
      "Epoch:  12\n",
      "Val loss: tensor(0.0020)\n",
      "current learning rate: 0.005\n",
      "Epoch:  13\n",
      "current learning rate: 0.005\n",
      "Epoch:  14\n",
      "Val loss: tensor(0.0024)\n",
      "current learning rate: 0.005\n",
      "Epoch:  15\n",
      "current learning rate: 0.005\n",
      "Epoch:  16\n",
      "Val loss: tensor(0.0029)\n",
      "current learning rate: 0.005\n",
      "Epoch:  17\n",
      "current learning rate: 0.005\n",
      "Epoch:  18\n",
      "Val loss: tensor(0.0028)\n",
      "current learning rate: 0.005\n",
      "Epoch:  19\n",
      "current learning rate: 0.005\n",
      "Epoch:  20\n",
      "Val loss: tensor(0.0027)\n",
      "current learning rate: 0.005\n",
      "Epoch:  21\n",
      "current learning rate: 0.005\n",
      "Epoch:  22\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.005\n",
      "Epoch:  23\n",
      "current learning rate: 0.005\n",
      "Epoch:  24\n",
      "Val loss: tensor(0.0023)\n",
      "current learning rate: 0.005\n",
      "Epoch:  25\n",
      "current learning rate: 0.005\n",
      "Epoch:  26\n",
      "Val loss: tensor(0.0031)\n",
      "current learning rate: 0.005\n",
      "Epoch:  27\n",
      "current learning rate: 0.005\n",
      "Epoch:  28\n",
      "Val loss: tensor(0.0018)\n",
      "current learning rate: 0.005\n",
      "Epoch:  29\n",
      "current learning rate: 0.005\n",
      "Epoch:  30\n",
      "Val loss: tensor(0.0023)\n",
      "current learning rate: 0.005\n",
      "Epoch:  31\n",
      "current learning rate: 0.005\n",
      "Epoch:  32\n",
      "Val loss: tensor(0.0019)\n",
      "current learning rate: 0.005\n",
      "Epoch:  33\n",
      "current learning rate: 0.005\n",
      "Epoch:  34\n",
      "Val loss: tensor(0.0017)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  35\n",
      "current learning rate: 0.0025\n",
      "Epoch:  36\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  37\n",
      "current learning rate: 0.0025\n",
      "Epoch:  38\n",
      "Val loss: tensor(0.0015)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  39\n",
      "current learning rate: 0.0025\n",
      "Epoch:  40\n",
      "Val loss: tensor(0.0017)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  41\n",
      "current learning rate: 0.0025\n",
      "Epoch:  42\n",
      "Val loss: tensor(0.0024)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  43\n",
      "current learning rate: 0.0025\n",
      "Epoch:  44\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  45\n",
      "current learning rate: 0.0025\n",
      "Epoch:  46\n",
      "Val loss: tensor(0.0023)\n",
      "current learning rate: 0.0025\n",
      "Epoch:  47\n",
      "current learning rate: 0.0025\n",
      "Epoch:  48\n",
      "Val loss: tensor(0.0019)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  49\n",
      "current learning rate: 0.00125\n",
      "Epoch:  50\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  51\n",
      "current learning rate: 0.00125\n",
      "Epoch:  52\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  53\n",
      "current learning rate: 0.00125\n",
      "Epoch:  54\n",
      "Val loss: tensor(0.0025)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  55\n",
      "current learning rate: 0.00125\n",
      "Epoch:  56\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  57\n",
      "current learning rate: 0.00125\n",
      "Epoch:  58\n",
      "Val loss: tensor(0.0025)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  59\n",
      "current learning rate: 0.00125\n",
      "Epoch:  60\n",
      "Val loss: tensor(0.0016)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  61\n",
      "current learning rate: 0.00125\n",
      "Epoch:  62\n",
      "Val loss: tensor(0.0018)\n",
      "current learning rate: 0.00125\n",
      "Epoch:  63\n",
      "current learning rate: 0.00125\n",
      "Epoch:  64\n",
      "Val loss: tensor(0.0015)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  65\n",
      "current learning rate: 0.000625\n",
      "Epoch:  66\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  67\n",
      "current learning rate: 0.000625\n",
      "Epoch:  68\n",
      "Val loss: tensor(0.0021)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  69\n",
      "current learning rate: 0.000625\n",
      "Epoch:  70\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  71\n",
      "current learning rate: 0.000625\n",
      "Epoch:  72\n",
      "Val loss: tensor(0.0015)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  73\n",
      "current learning rate: 0.000625\n",
      "Epoch:  74\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  75\n",
      "current learning rate: 0.000625\n",
      "Epoch:  76\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.000625\n",
      "Epoch:  77\n",
      "current learning rate: 0.000625\n",
      "Epoch:  78\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  79\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  80\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  81\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  82\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  83\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  84\n",
      "Val loss: tensor(0.0015)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  85\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  86\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  87\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  88\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  89\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  90\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  91\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  92\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  93\n",
      "current learning rate: 0.0003125\n",
      "Epoch:  94\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  95\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  96\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  97\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  98\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  99\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  100\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  101\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  102\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  103\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  104\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  105\n",
      "current learning rate: 0.00015625\n",
      "Epoch:  106\n",
      "Val loss: tensor(0.0015)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  107\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  108\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  109\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  110\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  111\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  112\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  113\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  114\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  115\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  116\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  117\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  118\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  119\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  120\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  121\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  122\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  123\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  124\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  125\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  126\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  127\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  128\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  129\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  130\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  131\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  132\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  133\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  134\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  135\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  136\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  137\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  138\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  139\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  140\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  141\n",
      "current learning rate: 7.8125e-05\n",
      "Epoch:  142\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  143\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  144\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  145\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  146\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  147\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  148\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  149\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  150\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  151\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  152\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  153\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  154\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  155\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  156\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  157\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  158\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  159\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  160\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  161\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  162\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  163\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  164\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  165\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  166\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  167\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  168\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  169\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  170\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  171\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  172\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  173\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  174\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  175\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  176\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  177\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  178\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  179\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  180\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  181\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  182\n",
      "Val loss: tensor(0.0014)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  183\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  184\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  185\n",
      "current learning rate: 3.90625e-05\n",
      "Epoch:  186\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  187\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  188\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  189\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  190\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  191\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  192\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  193\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  194\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  195\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  196\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  197\n",
      "current learning rate: 1.953125e-05\n",
      "Epoch:  198\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  199\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  200\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  201\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  202\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  203\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  204\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  205\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  206\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  207\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  208\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  209\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  210\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  211\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  212\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  213\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  214\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  215\n",
      "current learning rate: 9.765625e-06\n",
      "Epoch:  216\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  217\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  218\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  219\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  220\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  221\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  222\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  223\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  224\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  225\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  226\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  227\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  228\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  229\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  230\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  231\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  232\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  233\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  234\n",
      "Val loss: tensor(0.0013)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  235\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  236\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  237\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  238\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  239\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  240\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  241\n",
      "current learning rate: 4.8828125e-06\n",
      "Epoch:  242\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  243\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  244\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  245\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  246\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  247\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  248\n",
      "Val loss: tensor(0.0011)\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  249\n",
      "current learning rate: 2.44140625e-06\n",
      "Epoch:  250\n",
      "Val loss: tensor(0.0012)\n",
      "current learning rate: 2.44140625e-06\n",
      "unimplemented audio data type conversion...\n",
      "unimplemented audio data type conversion...\n",
      "Done Training\n",
      "Testing the final Model\n",
      "Testing the best Model\n",
      "Finished Training: SimpleRNNOCD_Parameterized_LSTM_hs32_pre_high_pass\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:50:54.404924: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 15:50:55.767350: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "c:\\Python312\\Lib\\site-packages\\torch\\__init__.py:696: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ..\\torch\\csrc\\tensor\\python_tensor.cpp:453.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "c:\\Python312\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "!python dist_model_recnet.py -l LSTM-32-OCD_Parameterized -eps 250 --seed 39 -lm False -is 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
